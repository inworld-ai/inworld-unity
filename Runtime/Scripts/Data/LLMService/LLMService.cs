/*************************************************************************************************
 * Copyright 2022-2024 Theai, Inc. dba Inworld AI
 *
 * Use of this source code is governed by the Inworld.ai Software Development Kit License Agreement
 * that can be found in the LICENSE.md file or at https://www.inworld.ai/sdk-license
 *************************************************************************************************/

using Newtonsoft.Json;
using Newtonsoft.Json.Converters;
using System;
using System.Collections.Generic;
using UnityEngine;

namespace Inworld.Entities.LLM
{
	[Serializable]
	public class Model
	{
		[Tooltip("The name of the model being served.")]
		public string model;
		
		[Tooltip("Service provider hosting llm and handling completion requests.")]
		[JsonConverter(typeof(StringEnumConverter))]
		public ServiceProvider service_provider;
		
		public Model(string model = "inworld-dragon")
		{
			this.model = model;
			if (model.StartsWith("inworld"))
				service_provider = ServiceProvider.SERVICE_PROVIDER_INWORLD;
			else if (model == "gpt-3.5-turbo-instruct" || model == "gpt-35-turbo-0613")
				service_provider = ServiceProvider.SERVICE_PROVIDER_AZURE;
			else
				service_provider = ServiceProvider.SERVICE_PROVIDER_OPENAI;
		}
	}
	[Serializable][Tooltip("Describes the serving ID of the request to select the right model.")]
	public class Serving
	{
		[Tooltip("ID of the model to use.")]
		public Model model_id;
		[Tooltip("Unique identifier representing end-user.")]
		public string user_id;

		[Tooltip("Unique identifier of the session with multiple completion requests.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public string session_id;

		public Serving()
		{
			user_id = "user-test";
			model_id = new Model();
		}
		public Serving(string modelName = "inworld-dragon", string userID = "user-test")
		{
			user_id = userID;
			model_id = new Model(modelName);
		}
	}
	
	[Serializable][Tooltip("Modify the likelihood of specified tokens appearing in the completion.")]
	public class LogitBias 
	{
		[Tooltip("Id of the token.")]
		public string token_id;
		[Range(-100, 100)][Tooltip("Bias value from -100 to 100.")]
		public int bias_value;
	}
	[Serializable]
	public class TextGenerationConfig
	{
		[Tooltip("Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Defaults to 0.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public float frequency_penalty = 0;
		
		[Tooltip("Modify the likelihood of specified tokens appearing in the completion. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public List<LogitBias> logit_bias;

		[Tooltip("Maximum number of output tokens allowed to generate. The total length of input tokens and generated tokens is limited by the model's context length. Defaults to inf.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public int max_tokens = 2500; //YAN: In our default LLM. The MaxToken is 2500. In OpenAI or others, it may be larger.
		
		[Tooltip("How many choices to generate for each input message. Defaults to 1.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public int n = 1;
		
		[Tooltip("Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public float presence_penalty = 0;
		
		[Tooltip("Up to 4 sequences where the API will stop generating further tokens.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public string stop;
		
		[Tooltip("If set, partial message deltas will be sent. Defaults to false.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public bool stream = false;
		
		[Tooltip("What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1.")]
		[Range(0,2)][JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public float temperature = 1;
		
		[Tooltip("An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public float top_p = 1;
		
		[Tooltip("Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens. The value must be strictly positive. Defaults to 1 (no penalty).")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public float repetition_penalty = 1;
		
		[Tooltip("Random seed for decoding.")]
		[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]
		public int seed;
	}
	
	[Serializable][Tooltip("Usage statistics for the completion request.")]
	public class Usage 
	{
		[Tooltip("Number of tokens in the generated completion.")]
		public int completion_tokens;
		[Tooltip("Number of tokens in the prompt.")]
		public int prompt_tokens;
	}
}
